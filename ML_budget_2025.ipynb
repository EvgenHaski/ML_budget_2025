{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ebcedc0-5f62-4bb6-b9d2-d6a9bcd7753b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\users\\shatrov_eo\\AppData\\Local\\Temp\\ipykernel_23144\\4185527183.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_predict[col] = le.transform(df_predict[col].astype(str))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ñ∂ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è postavka...\n",
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n",
      "–õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {'subsample': np.float64(0.8), 'reg_lambda': np.float64(0.1111111111111111), 'reg_alpha': np.float64(0.6666666666666666), 'num_leaves': np.int64(120), 'n_estimators': np.int64(200), 'min_child_samples': np.int64(25), 'max_depth': np.int64(12), 'learning_rate': np.float64(0.01), 'colsample_bytree': np.float64(0.5)}\n",
      "‚úÖ postavka: min=-16477538.62, max=1154459241.99, NaN=0\n",
      "‚ñ∂ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è postavka_1pal...\n",
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n",
      "–õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {'subsample': np.float64(0.9), 'reg_lambda': np.float64(0.8888888888888888), 'reg_alpha': np.float64(0.3333333333333333), 'num_leaves': np.int64(100), 'n_estimators': np.int64(250), 'min_child_samples': np.int64(25), 'max_depth': np.int64(12), 'learning_rate': np.float64(0.3), 'colsample_bytree': np.float64(0.8)}\n",
      "‚úÖ postavka_1pal: min=-3131.29, max=62489.31, NaN=0\n",
      "‚ñ∂ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è revenue...\n",
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n",
      "–õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {'subsample': np.float64(0.8), 'reg_lambda': np.float64(0.1111111111111111), 'reg_alpha': np.float64(0.6666666666666666), 'num_leaves': np.int64(120), 'n_estimators': np.int64(200), 'min_child_samples': np.int64(25), 'max_depth': np.int64(12), 'learning_rate': np.float64(0.01), 'colsample_bytree': np.float64(0.5)}\n",
      "‚úÖ revenue: min=12002.55, max=8126186.48, NaN=0\n",
      "‚ñ∂ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è revenue_1pal...\n",
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n",
      "–õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {'subsample': np.float64(1.0), 'reg_lambda': np.float64(0.3333333333333333), 'reg_alpha': np.float64(0.4444444444444444), 'num_leaves': np.int64(140), 'n_estimators': np.int64(200), 'min_child_samples': np.int64(15), 'max_depth': np.int64(6), 'learning_rate': np.float64(0.01), 'colsample_bytree': np.float64(0.6)}\n",
      "‚úÖ revenue_1pal: min=688.36, max=62307.47, NaN=0\n",
      "‚ñ∂ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è cost_transp...\n",
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n",
      "–õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {'subsample': np.float64(0.8), 'reg_lambda': np.float64(0.1111111111111111), 'reg_alpha': np.float64(0.6666666666666666), 'num_leaves': np.int64(120), 'n_estimators': np.int64(200), 'min_child_samples': np.int64(25), 'max_depth': np.int64(12), 'learning_rate': np.float64(0.01), 'colsample_bytree': np.float64(0.5)}\n",
      "‚úÖ cost_transp: min=-397510.20, max=2392652.56, NaN=0\n",
      "‚ñ∂ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è cost_transp_1pal...\n",
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n",
      "–õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {'subsample': np.float64(1.0), 'reg_lambda': np.float64(0.3333333333333333), 'reg_alpha': np.float64(0.4444444444444444), 'num_leaves': np.int64(140), 'n_estimators': np.int64(200), 'min_child_samples': np.int64(15), 'max_depth': np.int64(6), 'learning_rate': np.float64(0.01), 'colsample_bytree': np.float64(0.6)}\n",
      "‚úÖ cost_transp_1pal: min=131.71, max=10579.34, NaN=0\n",
      "‚ñ∂ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è cost_sklad...\n",
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n",
      "–õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {'subsample': np.float64(0.8), 'reg_lambda': np.float64(0.6666666666666666), 'reg_alpha': np.float64(0.2222222222222222), 'num_leaves': np.int64(130), 'n_estimators': np.int64(50), 'min_child_samples': np.int64(30), 'max_depth': np.int64(8), 'learning_rate': np.float64(0.29), 'colsample_bytree': np.float64(0.8)}\n",
      "‚úÖ cost_sklad: min=-440396.15, max=9775654.72, NaN=0\n",
      "‚ñ∂ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è cost_sklad_1pal...\n",
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n",
      "–õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {'subsample': np.float64(1.0), 'reg_lambda': np.float64(0.3333333333333333), 'reg_alpha': np.float64(0.4444444444444444), 'num_leaves': np.int64(140), 'n_estimators': np.int64(200), 'min_child_samples': np.int64(15), 'max_depth': np.int64(6), 'learning_rate': np.float64(0.01), 'colsample_bytree': np.float64(0.6)}\n",
      "‚úÖ cost_sklad_1pal: min=-322.86, max=3910.40, NaN=0\n",
      "‚ñ∂ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è sku_count...\n",
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n",
      "–õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {'subsample': np.float64(0.5), 'reg_lambda': np.float64(0.8888888888888888), 'reg_alpha': np.float64(0.6666666666666666), 'num_leaves': np.int64(50), 'n_estimators': np.int64(150), 'min_child_samples': np.int64(25), 'max_depth': np.int64(3), 'learning_rate': np.float64(0.12999999999999998), 'colsample_bytree': np.float64(0.6)}\n",
      "‚úÖ sku_count: min=-1.34, max=449.83, NaN=0\n",
      "‚ñ∂ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è avg_sku_price...\n",
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n",
      "–õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {'subsample': np.float64(0.8), 'reg_lambda': np.float64(0.1111111111111111), 'reg_alpha': np.float64(0.6666666666666666), 'num_leaves': np.int64(120), 'n_estimators': np.int64(200), 'min_child_samples': np.int64(25), 'max_depth': np.int64(12), 'learning_rate': np.float64(0.01), 'colsample_bytree': np.float64(0.5)}\n",
      "‚úÖ avg_sku_price: min=-7.16, max=4958.34, NaN=0\n",
      "‚ñ∂ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è effect (–≤–∞–ª–∏–¥–∞—Ü–∏—è)...\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "–õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {'subsample': np.float64(0.9), 'reg_lambda': np.float64(0.1111111111111111), 'reg_alpha': np.float64(1.0), 'num_leaves': np.int64(90), 'n_estimators': np.int64(250), 'min_child_samples': np.int64(5), 'max_depth': np.int64(4), 'learning_rate': np.float64(0.09999999999999998), 'colsample_bytree': np.float64(1.0)}\n",
      "üìâ RMSE –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: 194601.6281\n",
      "‚ñ∂ –§–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è effect –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö...\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "–õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {'subsample': np.float64(0.9), 'reg_lambda': np.float64(0.1111111111111111), 'reg_alpha': np.float64(1.0), 'num_leaves': np.int64(90), 'n_estimators': np.int64(250), 'min_child_samples': np.int64(5), 'max_depth': np.int64(4), 'learning_rate': np.float64(0.09999999999999998), 'colsample_bytree': np.float64(1.0)}\n",
      "‚úÖ–§–∞–π–ª —Å–æ–∑–¥–∞–Ω\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",

    "df = pd.read_excel(r\"e:\\users\\shatrov_eo\\Desktop\\–î–∞–Ω–Ω—ã–µ –¥–ª—è ML.xlsx\")\n",
    "df_2025 = pd.read_excel(r\"e:\\users\\shatrov_eo\\Desktop\\–î–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥–∏–∫—Ç 2025.xlsx\")\n",
    "\n",
    "df = df[df['year'] < 2025]  # —Ç–æ–ª—å–∫–æ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
    "\n",

    "known_features_2025 = [\n",
    "    'supplier_code', 'year', 'month', 'category', 'group',\n",
    "    'type_of_compensation', 'pul', 'dz', 'new', 'inf'\n",
    "]\n",
    "\n",
    "missing_features = [\n",
    "    'postavka', 'postavka_1pal', 'revenue', 'revenue_1pal',\n",
    "    'cost_transp', 'cost_transp_1pal', 'cost_sklad', 'cost_sklad_1pal',\n",
    "    'sku_count', 'avg_sku_price'\n",
    "]\n",
    "\n",
    "target_column = 'effect'\n",
    "binary_cols = ['pul', 'dz', 'new']\n",
    "df[binary_cols] = df[binary_cols].astype(int)\n",
    "df_2025[binary_cols] = df_2025[binary_cols].astype(int)\n",
    "\n",

    "categorical_cols = ['supplier_code', 'category', 'group', 'type_of_compensation']\n",
    "\n",
    "def encode_shared(df_train, df_predict, cat_cols):\n",
    "    encoders = {}\n",
    "    for col in cat_cols:\n",
    "        le = LabelEncoder()\n",
    "        df_train[col] = le.fit_transform(df_train[col].astype(str))\n",
    "        known_values = set(le.classes_)\n",
    "        df_predict = df_predict[df_predict[col].astype(str).isin(known_values)]\n",
    "        df_predict[col] = le.transform(df_predict[col].astype(str))\n",
    "        encoders[col] = le\n",
    "    return df_predict\n",
    "\n",
    "df_2025 = encode_shared(df, df_2025, categorical_cols)\n",
    "\n",

    "def add_features(data):\n",
    "    data['quarter'] = ((data['month'] - 1) // 3 + 1).astype(int)\n",
    "    data['pul_dz_interaction'] = data['pul'] * data['dz']\n",
    "    return data\n",
    "\n",
    "df = add_features(df)\n",
    "df_2025 = add_features(df_2025)\n",
    "\n",
    "def add_group_stats(data, ref_data, features, group_cols=['supplier_code', 'group']):\n",
    "    for feature in features:\n",
    "        for grp_col in group_cols:\n",
    "            mean_dict = ref_data.groupby(grp_col)[feature].mean()\n",
    "            col_name = f'{feature}_{grp_col}_mean'\n",
    "            data[col_name] = data[grp_col].map(mean_dict).fillna(0)\n",
    "    return data\n",
    "\n",
    "df = add_group_stats(df, df, [target_column] + missing_features)\n",
    "df_2025 = add_group_stats(df_2025, df, [target_column] + missing_features)\n",
    "\n",
    "extended_known_features = known_features_2025 + [\n",
    "    'quarter', 'pul_dz_interaction'\n",
    "]\n",
    "for feature in [target_column] + missing_features:\n",
    "    for grp in ['supplier_code', 'group']:\n",
    "        extended_known_features.append(f'{feature}_{grp}_mean')\n",
    "\n",
    "# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ LightGBM —Å RandomizedSearch \n",
    "def train_lgb_with_hyperopt(X, y, n_iter=30, random_state=42):\n",
    "    param_dist = {\n",
    "        'num_leaves': np.arange(20, 150, 10),\n",
    "        'max_depth': np.arange(3, 15, 1),\n",
    "        'learning_rate': np.linspace(0.01, 0.3, 30),\n",
    "        'n_estimators': np.arange(50, 300, 50),\n",
    "        'min_child_samples': np.arange(5, 50, 5),\n",
    "        'subsample': np.linspace(0.5, 1.0, 6),\n",
    "        'colsample_bytree': np.linspace(0.5, 1.0, 6),\n",
    "        'reg_alpha': np.linspace(0, 1, 10),\n",
    "        'reg_lambda': np.linspace(0, 1, 10)\n",
    "    }\n",
    "\n",
    "    model = lgb.LGBMRegressor(random_state=random_state, verbosity=-1)\n",
    "\n",
    "    rand_search = RandomizedSearchCV(\n",
    "        model, param_distributions=param_dist,\n",
    "        n_iter=n_iter, cv=3, scoring='neg_mean_squared_error',\n",
    "        verbose=1, random_state=random_state, n_jobs=-1\n",
    "    )\n",
    "\n",
    "    rand_search.fit(X, y)\n",
    "    print(\"–õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:\", rand_search.best_params_)\n",
    "    return rand_search.best_estimator_\n",
    "\n",
    "# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ \n",
    "predicted_features_2025 = df_2025[known_features_2025].copy()\n",
    "\n",
    "for feature in missing_features:\n",
    "    print(f\"‚ñ∂ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è {feature}...\")\n",
    "    X_train = df[extended_known_features]\n",
    "    y_train = df[feature]\n",
    "\n",
    "    model = train_lgb_with_hyperopt(X_train, y_train, n_iter=25)\n",
    "    preds = model.predict(df_2025[extended_known_features])\n",
    "    predicted_features_2025[feature] = pd.Series(preds, index=df_2025.index)\n",
    "\n",
    "    print(f\"‚úÖ {feature}: min={preds.min():.2f}, max={preds.max():.2f}, NaN={pd.isna(preds).sum()}\")\n",
    "\n",
    "# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ predicted_features_2025\n",
    "predicted_features_2025 = add_features(predicted_features_2025)\n",
    "predicted_features_2025 = add_group_stats(predicted_features_2025, df, [target_column] + missing_features)\n",
    "\n",

    "full_features_2024 = df[extended_known_features + missing_features]\n",
    "target_2024 = df[target_column]\n",
    "full_features_2025 = predicted_features_2025.copy()\n",
    "\n",

    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    full_features_2024, target_2024, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"‚ñ∂ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è effect (–≤–∞–ª–∏–¥–∞—Ü–∏—è)...\")\n",
    "effect_model_val = train_lgb_with_hyperopt(X_train, y_train, n_iter=30)\n",
    "val_preds = effect_model_val.predict(X_val)\n",
    "rmse = np.sqrt(mean_squared_error(y_val, val_preds))\n",
    "print(f\"üìâ RMSE –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {rmse:.4f}\")\n",
    "\n",
    "# –§–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "print(\"‚ñ∂ –§–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è effect –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö...\")\n",
    "effect_model = train_lgb_with_hyperopt(full_features_2024, target_2024, n_iter=30)\n",
    "\n",
    "# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ 2025\n",
    "df_2025['predicted_effect'] = effect_model.predict(full_features_2025)\n",
    "\n",

    "df_2025.to_excel(r\"e:\\users\\shatrov_eo\\Desktop\\–ø—Ä–µ–¥–∏–∫—Ç 2025.xlsx\", index=False)\n",
    "print('‚úÖ–§–∞–π–ª —Å–æ–∑–¥–∞–Ω')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191f5e89-897a-44d5-ab21-b4ab634973bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
